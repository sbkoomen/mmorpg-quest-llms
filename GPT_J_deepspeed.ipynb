{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "dfe6cec6d6cc42cdaa7da278750723a7",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": [
    "# Fine-tuning GPT-J in Jupyter"
   ],
   "block_group": "90d37317c3aa46648dd0371050943258"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "893108f263f049eb84f81c0aea8dae64",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "This code was originally setup for a Jupyter Labs environment on an Nvidia A10 GPU (24GB of VRAM) with 256GB of RAM."
   ],
   "block_group": "87f1d0e2ccb64315a44c69b62569a566"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "67e9251428fb4dc3bdcbd77d90e9c504",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "Find and replace \"DATASET\" with your dataset name."
   ],
   "block_group": "cd29bb6bd7304b5f972a63aa9bdd1e83"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "output_cleared": true,
    "execution_start": 1705059978484,
    "execution_millis": 1824,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "5f77171cf1b84041ace8e73270adf17a",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# install dependencies\n",
    "!pip install -r requirements.txt"
   ],
   "block_group": "22156f23b3a64a11b20ef8dad3cdc070",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "^C\nTraceback (most recent call last):\n  File \"/root/venv/bin/pip\", line 5, in <module>\n    from pip._internal.cli.main import main\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 9, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n    from pip._internal.cli.main_parser import create_main_parser\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n    from pip._internal.build_env import get_runnable_pip\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n    from pip._internal.cli.spinners import open_spinner\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n    from pip._internal.utils.logging import get_indentation\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n    from pip._internal.utils.misc import ensure_dir\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/utils/misc.py\", line 43, in <module>\n    from pip._internal.locations import get_major_minor_version\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/locations/__init__.py\", line 14, in <module>\n    from . import _sysconfig\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/locations/_sysconfig.py\", line 11, in <module>\n    from .base import change_root, get_major_minor_version, is_osx_framework\n  File \"/root/venv/lib/python3.9/site-packages/pip/_internal/locations/base.py\", line 16, in <module>\n    site_packages: str = sysconfig.get_path(\"purelib\")\n  File \"/usr/local/lib/python3.9/sysconfig.py\", line 524, in get_path\n    return get_paths(scheme, vars, expand)[name]\n  File \"/usr/local/lib/python3.9/sysconfig.py\", line 514, in get_paths\n    return _expand_vars(scheme, vars)\n  File \"/usr/local/lib/python3.9/sysconfig.py\", line 175, in _expand_vars\n    _extend_dict(vars, get_config_vars())\n  File \"/usr/local/lib/python3.9/sysconfig.py\", line 564, in get_config_vars\n    _init_posix(_CONFIG_VARS)\n  File \"/usr/local/lib/python3.9/sysconfig.py\", line 430, in _init_posix\n    _temp = __import__(name, globals(), locals(), ['build_time_vars'], 0)\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 157, in __enter__\n  File \"<frozen importlib._bootstrap>\", line 183, in _get_module_lock\nKeyboardInterrupt\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "source_hash": null,
    "execution_start": 1705059980315,
    "execution_millis": 17,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "839000992aab41c8a2f59e7431858720",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# set up local variables for deepspeed GPU(s) to use deepspeed in notebook, this emulates a launcher\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # order CUDA devices (GPUs) by bus ID\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # which CUDA device(s) to use"
   ],
   "block_group": "cc97a48ff3d04e63b423da015eb22675",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "source_hash": null,
    "output_cleared": true,
    "execution_start": 1705059980322,
    "execution_millis": 4579,
    "deepnote_to_be_reexecuted": false,
    "cell_id": "86060190f76f43ed8df8d362d92861aa",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy"
   ],
   "block_group": "546d67605e37495dac7e3b4aa8617ffe",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, random_split\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "output_cleared": true,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "d340e4d58f4b4fe5bf4336fb86ca59bb",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# check if GPU is available and how much VRAM is available\n",
    "!nvidia-smi"
   ],
   "block_group": "28f45fe4a1b34b17bd4e9a80ebc95f99",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sun Nov 12 20:27:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A10          On   | 00000000:17:00.0 Off |                    0 |\n|  0%   34C    P8    16W / 150W |      0MiB / 23028MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A10          On   | 00000000:CA:00.0 Off |                    0 |\n|  0%   56C    P8    17W / 150W |      0MiB / 23028MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "96ef6d7817b64e65880a19ab388b6265",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": [
    "# Making deepspeed config file"
   ],
   "block_group": "80f8f84063d94ee4a08500328d22714b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 88,
      "fromCodePoint": 71
     }
    ],
    "cell_id": "54bece03f32546b1be2faa27ba4af24d",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "Make sure the following json config parameters are consistent with the TrainingArguments in the next code block:"
   ],
   "block_group": "d74ece2aa26147dcb1525b59b8eb0b24"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 16,
      "fromCodePoint": 0
     },
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 49,
      "fromCodePoint": 22
     }
    ],
    "cell_id": "ef37f2353dd74388acf2312edd0909bb",
    "deepnote_cell_type": "text-cell-bullet"
   },
   "source": [
    "- train_batch_size <==> per_device_train_batch_size"
   ],
   "block_group": "f6580fb9866d4196b9e37278528858f0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 3,
      "fromCodePoint": 0
     },
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 21,
      "fromCodePoint": 8
     }
    ],
    "cell_id": "8131fca456894bcda363a4a9c3ae05f0",
    "deepnote_cell_type": "text-cell-bullet"
   },
   "source": [
    "- lr <==> learning_rate"
   ],
   "block_group": "54ce686aec3642c0bd9bd7246d47b179"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 17,
      "fromCodePoint": 0
     },
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 34,
      "fromCodePoint": 22
     }
    ],
    "cell_id": "a6a1f4931c814894a7b77f070728be1e",
    "deepnote_cell_type": "text-cell-bullet"
   },
   "source": [
    "- warmup_num_steps <==> warmup_steps"
   ],
   "block_group": "22f3a6abc3504851865006480cc4c690"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [
     {
      "type": "marks",
      "marks": {
       "italic": true
      },
      "toCodePoint": 170,
      "fromCodePoint": 166
     }
    ],
    "cell_id": "01d0367c2ed04d60986813db132df81a",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "Sorry for the inconvenience of manually keeping these consistent, I couldn't find a way to automate/remove this quickly.\n",
    "It might be possible to set some of these to auto but I didn't bother to find out, the parameters didn't change much during my research."
   ],
   "block_group": "fa1e51a7e06f4a598fa445266c8d17e9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "d446ffd381b141f79ab66662085aad61",
    "deepnote_cell_type": "text-cell-p"
   },
   "source": [
    "If this code block doesn't work, you can also just make the json file yourself by copying the curly-bracketed text."
   ],
   "block_group": "6bfa585954e847f38078582ba4231b2a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "360c339a140b4a38827fd51a636464e8",
    "deepnote_cell_type": "code"
   },
   "source": [
    "%%bash\n",
    "cat <<'EOT' > ds_config_gpt_j.json\n",
    "{\n",
    "  \"resume_from_checkpoint\": true,\n",
    "  \"train_batch_size\": 1,\n",
    "  \"bf16\": {\n",
    "    \"enabled\": true,\n",
    "    \"min_loss_scale\": 0.25,\n",
    "    \"opt_level\": \"O3\"\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"contiguous_gradients\": true\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 1e-05,\n",
    "      \"betas\": [\n",
    "        0.9,\n",
    "        0.999\n",
    "      ],\n",
    "      \"eps\": 1e-08\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": 0,\n",
    "      \"warmup_max_lr\": 1e-05,\n",
    "      \"warmup_num_steps\": 45\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "EOT"
   ],
   "block_group": "e8436bd0d22b45bf9faeea440be76c7d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "64ebe052d3634184a7e7e4a556c04b24",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": [
    "# Setup model fine-tuning parameters and dataset"
   ],
   "block_group": "dfad5d4870534c75951ab3849c80a351"
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "output_cleared": true,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "489a77baa5df40dfb4b29f19acade9e9",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# same seed for consistent results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# setup GPT-J tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "# set arguments for training, see https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments \n",
    "# batch size of 1 or 2 is recommended with 24GB of VRAM and a dataset of ~6k entries\n",
    "# warmup_steps should be ~10% of dataset size, maybe try another scheduler for better results, see https://www.deepspeed.ai/docs/config-json/#scheduler-parameters \n",
    "# weight decay avoids overfitting, 0.01 to 0.1 is recommended\n",
    "# bf16 means bfloat16 is enabled, see https://cloud.google.com/tpu/docs/bfloat16\n",
    "\n",
    "# check consistency of TrainingArguments with deepspeed json config file above!\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=5, gradient_accumulation_steps=1, learning_rate=1e-05,\n",
    "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1, warmup_steps=45,\n",
    "                                  weight_decay=0.1, logging_dir='./logs', bf16=True, deepspeed='./ds_config_gpt_j.json')\n",
    "\n",
    "# setup GPT-J model to fine-tune\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\").cuda()\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# loading CSV file, replace \"DATASET\" with name of CSV file with tags inserted\n",
    "descriptions = pd.read_csv('DATASET.csv', sep='\\t')['descriptions']\n",
    "\n",
    "# get max length of entries in dataset, required for knowing how much to pad each entry later\n",
    "max_length = max([len(tokenizer.encode(description)) for description in descriptions])\n",
    "print(\"Max length: {}\".format(max_length))"
   ],
   "block_group": "b7af9ca14f0341a4bee1d37b775ae0d6",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-26 12:35:46,511] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n[2023-11-26 12:38:43,146] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 6.05B parameters\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/jovyan/venvs/my_environment/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n  warnings.warn(\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Max length: 323\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "43931ce6215547538651594658024ef9",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# set up dataset class for convenience\n",
    "class GameDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ],
   "block_group": "e1c67eb484654552ba97122e5554c60c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "formattedRanges": [],
    "cell_id": "38f37ec5b2f148acb266bc2adfd21b50",
    "deepnote_cell_type": "text-cell-h1"
   },
   "source": [
    "# Training"
   ],
   "block_group": "ae283724b9e44cbeafc6915164a78fe7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "source_hash": null,
    "output_cleared": true,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "4309cf8e5bd1432b978edbe73221b8b1",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# set up dataset, split for 90% training, 10% validation \n",
    "# not actually sure if validation ever gets used because the documentation is vague\n",
    "dataset = GameDataset(descriptions, tokenizer, max_length=max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# set up deepspeed training\n",
    "model_trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])})\n",
    "model_trainer.train()\n",
    "\n",
    "# save model after training\n",
    "model_trainer.save_model(\"./gpt-j-DATASET\")"
   ],
   "block_group": "8db70ad6845c4729867c996cde6e436e",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using cuda_amp half precision backend\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 20:28:11,047] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed info: version=0.8.1, git-hash=unknown, git-branch=unknown\n[2023-11-12 20:28:11,121] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using /home/jovyan/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /home/jovyan/.cache/torch_extensions/py310_cu116/cpu_adam/build.ninja...\nBuilding extension module cpu_adam...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module cpu_adam...\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ninja: no work to do.\nTime to load cpu_adam op: 2.9119908809661865 seconds\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\nConfig: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n[2023-11-12 20:28:17,550] [INFO] [logging.py:75:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n[2023-11-12 20:28:17,568] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n[2023-11-12 20:28:17,569] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n[2023-11-12 20:28:17,570] [INFO] [logging.py:75:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\n[2023-11-12 20:28:17,651] [INFO] [utils.py:825:see_memory_usage] Stage 3 initialize beginning\n[2023-11-12 20:28:17,653] [INFO] [utils.py:826:see_memory_usage] MA 0.88 GB         Max_MA 1.65 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:17,655] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 20.32 GB, percent = 8.1%\n[2023-11-12 20:28:17,658] [INFO] [stage3.py:114:__init__] Reduce bucket size 500,000,000\n[2023-11-12 20:28:17,659] [INFO] [stage3.py:115:__init__] Prefetch bucket size 50,000,000\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using /home/jovyan/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...\nEmitting ninja build file /home/jovyan/.cache/torch_extensions/py310_cu116/utils/build.ninja...\nBuilding extension module utils...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ninja: no work to do.\nTime to load utils op: 0.15554070472717285 seconds\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Loading extension module utils...\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 20:28:17,879] [INFO] [utils.py:825:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n[2023-11-12 20:28:17,881] [INFO] [utils.py:826:see_memory_usage] MA 0.88 GB         Max_MA 0.88 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:17,883] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 20.32 GB, percent = 8.1%\nParameter Offload: Total persistent parameters: 861410 in 115 params\n[2023-11-12 20:28:18,380] [INFO] [utils.py:825:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n[2023-11-12 20:28:18,382] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.88 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:18,384] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 21.09 GB, percent = 8.4%\n[2023-11-12 20:28:18,440] [INFO] [utils.py:825:see_memory_usage] Before creating fp16 partitions\n[2023-11-12 20:28:18,442] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:18,444] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 21.09 GB, percent = 8.4%\n[2023-11-12 20:28:28,267] [INFO] [utils.py:825:see_memory_usage] After creating fp16 partitions: 7\n[2023-11-12 20:28:28,269] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:28,272] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 26.22 GB, percent = 10.4%\n[2023-11-12 20:28:28,327] [INFO] [utils.py:825:see_memory_usage] Before creating fp32 partitions\n[2023-11-12 20:28:28,330] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:28,331] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 26.22 GB, percent = 10.4%\n[2023-11-12 20:28:30,731] [INFO] [utils.py:825:see_memory_usage] After creating fp32 partitions\n[2023-11-12 20:28:30,733] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:30,736] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 48.77 GB, percent = 19.4%\n[2023-11-12 20:28:30,795] [INFO] [utils.py:825:see_memory_usage] Before initializing optimizer states\n[2023-11-12 20:28:30,796] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:30,799] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 48.75 GB, percent = 19.4%\n[2023-11-12 20:28:37,688] [INFO] [utils.py:825:see_memory_usage] After initializing optimizer states\n[2023-11-12 20:28:37,691] [INFO] [utils.py:826:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 1.93 GB         Max_CA 2 GB \n[2023-11-12 20:28:37,693] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 116.49 GB, percent = 46.3%\n[2023-11-12 20:28:37,694] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\n[2023-11-12 20:28:38,985] [INFO] [utils.py:825:see_memory_usage] After initializing ZeRO optimizer\n[2023-11-12 20:28:38,988] [INFO] [utils.py:826:see_memory_usage] MA 1.04 GB         Max_MA 1.81 GB         CA 2.86 GB         Max_CA 3 GB \n[2023-11-12 20:28:38,990] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 127.78 GB, percent = 50.8%\n[2023-11-12 20:28:38,991] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n[2023-11-12 20:28:38,992] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n[2023-11-12 20:28:38,993] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9613255cf0>\n[2023-11-12 20:28:38,994] [INFO] [logging.py:75:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:28:38,996] [INFO] [config.py:1009:print] DeepSpeedEngine configuration:\n[2023-11-12 20:28:38,997] [INFO] [config.py:1013:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2023-11-12 20:28:38,999] [INFO] [config.py:1013:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n[2023-11-12 20:28:38,999] [INFO] [config.py:1013:print]   amp_enabled .................. False\n[2023-11-12 20:28:38,999] [INFO] [config.py:1013:print]   amp_params ................... False\n[2023-11-12 20:28:39,000] [INFO] [config.py:1013:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2023-11-12 20:28:39,000] [INFO] [config.py:1013:print]   bfloat16_enabled ............. True\n[2023-11-12 20:28:39,001] [INFO] [config.py:1013:print]   checkpoint_parallel_write_pipeline  False\n[2023-11-12 20:28:39,001] [INFO] [config.py:1013:print]   checkpoint_tag_validation_enabled  True\n[2023-11-12 20:28:39,002] [INFO] [config.py:1013:print]   checkpoint_tag_validation_fail  False\n[2023-11-12 20:28:39,002] [INFO] [config.py:1013:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f973b40dc30>\n[2023-11-12 20:28:39,003] [INFO] [config.py:1013:print]   communication_data_type ...... None\n[2023-11-12 20:28:39,003] [INFO] [config.py:1013:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2023-11-12 20:28:39,004] [INFO] [config.py:1013:print]   curriculum_enabled_legacy .... False\n[2023-11-12 20:28:39,004] [INFO] [config.py:1013:print]   curriculum_params_legacy ..... False\n[2023-11-12 20:28:39,004] [INFO] [config.py:1013:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2023-11-12 20:28:39,005] [INFO] [config.py:1013:print]   data_efficiency_enabled ...... False\n[2023-11-12 20:28:39,005] [INFO] [config.py:1013:print]   dataloader_drop_last ......... False\n[2023-11-12 20:28:39,006] [INFO] [config.py:1013:print]   disable_allgather ............ False\n[2023-11-12 20:28:39,006] [INFO] [config.py:1013:print]   dump_state ................... False\n[2023-11-12 20:28:39,007] [INFO] [config.py:1013:print]   dynamic_loss_scale_args ...... None\n[2023-11-12 20:28:39,007] [INFO] [config.py:1013:print]   eigenvalue_enabled ........... False\n[2023-11-12 20:28:39,007] [INFO] [config.py:1013:print]   eigenvalue_gas_boundary_resolution  1\n[2023-11-12 20:28:39,008] [INFO] [config.py:1013:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2023-11-12 20:28:39,008] [INFO] [config.py:1013:print]   eigenvalue_layer_num ......... 0\n[2023-11-12 20:28:39,008] [INFO] [config.py:1013:print]   eigenvalue_max_iter .......... 100\n[2023-11-12 20:28:39,009] [INFO] [config.py:1013:print]   eigenvalue_stability ......... 1e-06\n[2023-11-12 20:28:39,009] [INFO] [config.py:1013:print]   eigenvalue_tol ............... 0.01\n[2023-11-12 20:28:39,009] [INFO] [config.py:1013:print]   eigenvalue_verbose ........... False\n[2023-11-12 20:28:39,009] [INFO] [config.py:1013:print]   elasticity_enabled ........... False\n[2023-11-12 20:28:39,010] [INFO] [config.py:1013:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2023-11-12 20:28:39,010] [INFO] [config.py:1013:print]   fp16_auto_cast ............... None\n[2023-11-12 20:28:39,011] [INFO] [config.py:1013:print]   fp16_enabled ................. False\n[2023-11-12 20:28:39,011] [INFO] [config.py:1013:print]   fp16_master_weights_and_gradients  False\n[2023-11-12 20:28:39,011] [INFO] [config.py:1013:print]   global_rank .................. 0\n[2023-11-12 20:28:39,012] [INFO] [config.py:1013:print]   grad_accum_dtype ............. None\n[2023-11-12 20:28:39,012] [INFO] [config.py:1013:print]   gradient_accumulation_steps .. 1\n[2023-11-12 20:28:39,012] [INFO] [config.py:1013:print]   gradient_clipping ............ 0.0\n[2023-11-12 20:28:39,013] [INFO] [config.py:1013:print]   gradient_predivide_factor .... 1.0\n[2023-11-12 20:28:39,013] [INFO] [config.py:1013:print]   initial_dynamic_scale ........ 1\n[2023-11-12 20:28:39,013] [INFO] [config.py:1013:print]   load_universal_checkpoint .... False\n[2023-11-12 20:28:39,014] [INFO] [config.py:1013:print]   loss_scale ................... 1.0\n[2023-11-12 20:28:39,014] [INFO] [config.py:1013:print]   memory_breakdown ............. False\n[2023-11-12 20:28:39,014] [INFO] [config.py:1013:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n[2023-11-12 20:28:39,015] [INFO] [config.py:1013:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2023-11-12 20:28:39,015] [INFO] [config.py:1013:print]   optimizer_legacy_fusion ...... False\n[2023-11-12 20:28:39,015] [INFO] [config.py:1013:print]   optimizer_name ............... adamw\n[2023-11-12 20:28:39,016] [INFO] [config.py:1013:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999], 'eps': 1e-08}\n[2023-11-12 20:28:39,016] [INFO] [config.py:1013:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n[2023-11-12 20:28:39,016] [INFO] [config.py:1013:print]   pld_enabled .................. False\n[2023-11-12 20:28:39,017] [INFO] [config.py:1013:print]   pld_params ................... False\n[2023-11-12 20:28:39,017] [INFO] [config.py:1013:print]   prescale_gradients ........... False\n[2023-11-12 20:28:39,018] [INFO] [config.py:1013:print]   scheduler_name ............... WarmupLR\n[2023-11-12 20:28:39,018] [INFO] [config.py:1013:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 22}\n[2023-11-12 20:28:39,018] [INFO] [config.py:1013:print]   sparse_attention ............. None\n[2023-11-12 20:28:39,019] [INFO] [config.py:1013:print]   sparse_gradients_enabled ..... False\n[2023-11-12 20:28:39,019] [INFO] [config.py:1013:print]   steps_per_print .............. 10\n[2023-11-12 20:28:39,019] [INFO] [config.py:1013:print]   train_batch_size ............. 4\n[2023-11-12 20:28:39,019] [INFO] [config.py:1013:print]   train_micro_batch_size_per_gpu  4\n[2023-11-12 20:28:39,020] [INFO] [config.py:1013:print]   use_node_local_storage ....... False\n[2023-11-12 20:28:39,020] [INFO] [config.py:1013:print]   wall_clock_breakdown ......... False\n[2023-11-12 20:28:39,020] [INFO] [config.py:1013:print]   world_size ................... 1\n[2023-11-12 20:28:39,021] [INFO] [config.py:1013:print]   zero_allow_untested_optimizer  False\n[2023-11-12 20:28:39,021] [INFO] [config.py:1013:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n[2023-11-12 20:28:39,022] [INFO] [config.py:1013:print]   zero_enabled ................. True\n[2023-11-12 20:28:39,022] [INFO] [config.py:1013:print]   zero_optimization_stage ...... 3\n[2023-11-12 20:28:39,022] [INFO] [config.py:998:print_user_config]   json = {\n    \"resume_from_checkpoint\": true, \n    \"train_batch_size\": 4, \n    \"bf16\": {\n        \"enabled\": true, \n        \"min_loss_scale\": 0.25, \n        \"opt_level\": \"O3\"\n    }, \n    \"zero_optimization\": {\n        \"stage\": 3, \n        \"offload_param\": {\n            \"device\": \"cpu\"\n        }, \n        \"offload_optimizer\": {\n            \"device\": \"cpu\"\n        }, \n        \"allgather_partitions\": true, \n        \"allgather_bucket_size\": 5.000000e+08, \n        \"contiguous_gradients\": true\n    }, \n    \"optimizer\": {\n        \"type\": \"AdamW\", \n        \"params\": {\n            \"lr\": 1e-05, \n            \"betas\": [0.9, 0.999], \n            \"eps\": 1e-08\n        }\n    }, \n    \"scheduler\": {\n        \"type\": \"WarmupLR\", \n        \"params\": {\n            \"warmup_min_lr\": 0, \n            \"warmup_max_lr\": 1e-05, \n            \"warmup_num_steps\": 22\n        }\n    }\n}\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using /home/jovyan/.cache/torch_extensions/py310_cu116 as PyTorch extensions root...\nNo modifications detected for re-loaded extension module utils, skipping build step...\nLoading extension module utils...\n***** Running training *****\n  Num examples = 900\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 225\n  Number of trainable parameters = 0\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Time to load utils op: 0.005967378616333008 seconds\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/jovyan/venvs/my_environment/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n  warnings.warn(\n/home/jovyan/venvs/my_environment/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n  warnings.warn(\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 1:12:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>5.896900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.692200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.589100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.410900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.428100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.367200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.310900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.364100</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.392200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.298400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.264100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.290600</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.342200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.357800</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.244500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.379700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.169500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.098400</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.143800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.178100</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.210900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.181200</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.102300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.226600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.348400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.206300</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.226600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.249200</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.228100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.149200</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.102300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.102300</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.292200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.167200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.185900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.279700</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.109400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.159400</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.210900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.254700</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>1.165600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.134400</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>1.278100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.153900</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.168700</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 20:32:07,466] [INFO] [logging.py:75:log_dist] [Rank 0] step=10, skipped=0, lr=[7.44921859773347e-06], mom=[[0.9, 0.999]]\n[2023-11-12 20:32:07,468] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=0.19143738813000266, CurrSamplesPerSec=0.18888465122617287, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:35:23,224] [INFO] [logging.py:75:log_dist] [Rank 0] step=20, skipped=0, lr=[9.691656839909223e-06], mom=[[0.9, 0.999]]\n[2023-11-12 20:35:23,226] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=0.1984423663521117, CurrSamplesPerSec=0.2064105363514331, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:38:35,825] [INFO] [logging.py:75:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:38:35,827] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=0.20167892837235188, CurrSamplesPerSec=0.21611128636033308, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:41:51,393] [INFO] [logging.py:75:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:41:51,394] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=0.20244613324545646, CurrSamplesPerSec=0.20140673657424457, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:45:06,292] [INFO] [logging.py:75:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:45:06,295] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=0.20303765913251673, CurrSamplesPerSec=0.24347063185830758, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:48:17,856] [INFO] [logging.py:75:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:48:17,858] [INFO] [timer.py:198:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=0.20402640068464897, CurrSamplesPerSec=0.21474094335299773, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:51:37,954] [INFO] [logging.py:75:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:51:37,955] [INFO] [timer.py:198:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=0.20342207830785283, CurrSamplesPerSec=0.18199276924060886, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:54:56,957] [INFO] [logging.py:75:log_dist] [Rank 0] step=80, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:54:56,958] [INFO] [timer.py:198:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=0.2031182224363773, CurrSamplesPerSec=0.20164509607531364, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 20:58:14,547] [INFO] [logging.py:75:log_dist] [Rank 0] step=90, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 20:58:14,549] [INFO] [timer.py:198:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=0.20304944840602904, CurrSamplesPerSec=0.20612890996241234, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:01:33,211] [INFO] [logging.py:75:log_dist] [Rank 0] step=100, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:01:33,213] [INFO] [timer.py:198:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=0.20288202437012606, CurrSamplesPerSec=0.20446531782960928, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:04:39,899] [INFO] [logging.py:75:log_dist] [Rank 0] step=110, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:04:39,901] [INFO] [timer.py:198:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=0.20389166929188512, CurrSamplesPerSec=0.2374178004339806, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:07:50,934] [INFO] [logging.py:75:log_dist] [Rank 0] step=120, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:07:50,936] [INFO] [timer.py:198:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=0.20435328667833555, CurrSamplesPerSec=0.19253111233225825, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:11:07,427] [INFO] [logging.py:75:log_dist] [Rank 0] step=130, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:11:07,429] [INFO] [timer.py:198:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=0.20429884748988236, CurrSamplesPerSec=0.2084280965805374, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:14:15,435] [INFO] [logging.py:75:log_dist] [Rank 0] step=140, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:14:15,437] [INFO] [timer.py:198:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=0.20489462701705813, CurrSamplesPerSec=0.22294594967825077, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:17:22,402] [INFO] [logging.py:75:log_dist] [Rank 0] step=150, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:17:22,404] [INFO] [timer.py:198:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=0.20548830303790647, CurrSamplesPerSec=0.21915912559495476, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:20:30,449] [INFO] [logging.py:75:log_dist] [Rank 0] step=160, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:20:30,450] [INFO] [timer.py:198:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=0.20593700252529498, CurrSamplesPerSec=0.20623386156606424, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:23:43,981] [INFO] [logging.py:75:log_dist] [Rank 0] step=170, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:23:43,984] [INFO] [timer.py:198:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=0.20598588646307742, CurrSamplesPerSec=0.19995044505890877, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:27:02,243] [INFO] [logging.py:75:log_dist] [Rank 0] step=180, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:27:02,245] [INFO] [timer.py:198:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=0.20574813044758838, CurrSamplesPerSec=0.19554734736858864, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:30:12,454] [INFO] [logging.py:75:log_dist] [Rank 0] step=190, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:30:12,456] [INFO] [timer.py:198:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=0.20598905794563402, CurrSamplesPerSec=0.20961928869445784, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:33:22,643] [INFO] [logging.py:75:log_dist] [Rank 0] step=200, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:33:22,645] [INFO] [timer.py:198:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=0.20620749133908897, CurrSamplesPerSec=0.20443099108091875, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:36:37,040] [INFO] [logging.py:75:log_dist] [Rank 0] step=210, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:36:37,042] [INFO] [timer.py:198:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=0.20619032126732859, CurrSamplesPerSec=0.21371639841105172, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n[2023-11-12 21:39:50,736] [INFO] [logging.py:75:log_dist] [Rank 0] step=220, skipped=0, lr=[1e-05], mom=[[0.9, 0.999]]\n[2023-11-12 21:39:50,739] [INFO] [timer.py:198:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=0.20620846306298948, CurrSamplesPerSec=0.2121244431362531, MemAllocated=1.04GB, MaxMemAllocated=14.53GB\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nSaving model checkpoint to ./gpt-j-wow-1ep-small\nConfiguration saved in ./gpt-j-wow-1ep-small/config.json\nConfiguration saved in ./gpt-j-wow-1ep-small/generation_config.json\nModel weights saved in ./gpt-j-wow-1ep-small/pytorch_model.bin\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 21:41:28,452] [INFO] [engine.py:3507:save_16bit_model] Did not save the model ./gpt-j-wow-1ep-small/pytorch_model.bin because `stage3_gather_16bit_weights_on_model_save` is False\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "deepspeed.save_16bit_model didn't save the model, since stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use zero_to_fp32.py to recover weights\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 21:41:28,474] [INFO] [logging.py:75:log_dist] [Rank 0] [Torch] Checkpoint global_step225 is begin to save!\n[2023-11-12 21:41:28,491] [INFO] [logging.py:75:log_dist] [Rank 0] Saving model checkpoint: ./gpt-j-wow-1ep-small/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt\n[2023-11-12 21:41:28,493] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving ./gpt-j-wow-1ep-small/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt...\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/jovyan/venvs/my_environment/lib/python3.10/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2023-11-12 21:41:28,837] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved ./gpt-j-wow-1ep-small/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt.\n[2023-11-12 21:41:28,840] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving ./gpt-j-wow-1ep-small/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2023-11-12 21:43:26,399] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved ./gpt-j-wow-1ep-small/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2023-11-12 21:43:26,501] [INFO] [engine.py:3407:_save_zero_checkpoint] zero checkpoint saved ./gpt-j-wow-1ep-small/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2023-11-12 21:43:26,520] [INFO] [torch_checkpoint_engine.py:27:commit] [Torch] Checkpoint global_step225 is ready now!\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": null,
    "deepnote_to_be_reexecuted": true,
    "cell_id": "f1f95d50b59c45888d52d967caea6905",
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "block_group": "834e82b6cf6d4aa9856ac7dc5e6a9e4f",
   "execution_count": null,
   "outputs": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2024-01-12T11:00:51.997Z"
  },
  "deepnote_notebook_id": "f92b181fe11a4cacb06c3f8ff4752437",
  "deepnote_execution_queue": []
 }
}